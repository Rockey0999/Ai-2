<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MoodMirror | Live AI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <style>
        canvas { position: absolute; top: 0; left: 0; }
        .scanning { position: relative; overflow: hidden; border: 4px solid #10b981; border-radius: 1rem; }
        .scanning::after {
            content: ""; position: absolute; top: 0; left: 0; width: 100%; height: 2px;
            background: #10b981; animation: scan 3s linear infinite; box-shadow: 0 0 15px #10b981;
        }
        @keyframes scan { 0% { top: 0%; } 100% { top: 100%; } }
    </style>
</head>
<body class="bg-slate-950 text-white min-h-screen flex flex-col items-center justify-center p-4">

    <div id="overlay-screen" class="fixed inset-0 z-50 bg-slate-900 flex flex-col items-center justify-center transition-opacity duration-500">
        <h1 class="text-4xl font-black mb-4 text-emerald-500 italic uppercase">MoodMirror</h1>
        <p class="mb-8 text-slate-400">Click below to initialize AI and Camera</p>
        <button onclick="initProject()" class="bg-emerald-500 hover:bg-emerald-400 text-slate-950 px-10 py-4 rounded-full font-bold text-xl shadow-lg shadow-emerald-500/20 transition-transform active:scale-95">
            START SESSION
        </button>
    </div>

    <div class="w-full max-w-4xl space-y-6">
        <div class="flex justify-between items-end">
            <div>
                <h2 class="text-xs font-bold text-emerald-500 uppercase tracking-widest">Live Vision Engine</h2>
                <p id="status-text" class="text-slate-500 text-sm italic italic">System Offline</p>
            </div>
            <button onclick="captureImage()" class="bg-slate-800 hover:bg-slate-700 px-6 py-2 rounded-lg text-xs font-bold border border-slate-700">CAPTURE SNAPSHOT</button>
        </div>

        <div class="scanning" id="container">
            <video id="video" width="1280" height="720" class="w-full h-auto bg-black" autoplay muted playsinline></video>
        </div>

        <div id="result-box" class="hidden bg-slate-900 border border-emerald-500/30 p-6 rounded-2xl flex items-center justify-between">
            <div>
                <p class="text-xs text-slate-500 uppercase font-bold">Mental State Detection</p>
                <h3 id="emotion-display" class="text-3xl font-black text-emerald-400 uppercase tracking-tighter">Analyzing...</h3>
            </div>
            <canvas id="snapshot" class="w-32 h-20 rounded-lg border border-slate-700"></canvas>
        </div>
    </div>

    <script>
        const video = document.getElementById('video');
        const status = document.getElementById('status-text');

        async function initProject() {
            document.getElementById('overlay-screen').style.opacity = '0';
            setTimeout(() => document.getElementById('overlay-screen').style.display = 'none', 500);
            
            status.innerText = "Loading AI Models...";
            
            // CORRECTED: Using 'faceapi' without hyphen and reliable HTTPS URL
            const MODEL_URL = 'https://justadudewhohacks.github.io/face-api.js/models';
            
            try {
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
                    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
                ]);
                
                status.innerText = "Models Ready. Accessing Camera...";
                startVideo();
            } catch (err) {
                status.innerText = "Error Loading Models. Check Connection.";
                console.error(err);
            }
        }

        function startVideo() {
            navigator.mediaDevices.getUserMedia({ video: {} })
                .then(stream => {
                    video.srcObject = stream;
                    status.innerText = "System Live - Monitoring Emotions";
                })
                .catch(err => {
                    status.innerText = "Camera Access Denied!";
                    alert("Please allow camera access in your browser settings.");
                });
        }

        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video);
            document.getElementById('container').append(canvas);
            const displaySize = { width: video.clientWidth, height: video.clientHeight };
            faceapi.matchDimensions(canvas, displaySize);

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                faceapi.draw.drawDetections(canvas, resizedDetections);
                
                if(detections.length > 0) {
                    const expressions = detections[0].expressions;
                    const topEmotion = Object.entries(expressions).reduce((a, b) => a[1] > b[1] ? a : b)[0];
                    document.getElementById('liveEmotion')?.innerText = topEmotion;
                }
            }, 150);
        });

        function captureImage() {
            const snapCanvas = document.getElementById('snapshot');
            const resultBox = document.getElementById('result-box');
            resultBox.classList.remove('hidden');
            
            snapCanvas.width = video.videoWidth;
            snapCanvas.height = video.videoHeight;
            snapCanvas.getContext('2d').drawImage(video, 0, 0);
            
            // Add a fun "analyzing" delay
            document.getElementById('emotion-display').innerText = "Processing...";
            setTimeout(async () => {
                const detection = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
                if(detection) {
                    const emotion = Object.entries(detection.expressions).reduce((a, b) => a[1] > b[1] ? a : b)[0];
                    document.getElementById('emotion-display').innerText = emotion;
                }
            }, 800);
        }
    </script>
</body>
</html>
